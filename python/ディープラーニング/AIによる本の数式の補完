# 数式の解説

## 第2章 パーセプトロン

### ANDゲートのパーセプトロン
ANDゲートは、2つの入力が共に1の時に出力が1となる論理ゲートです。パーセプトロンで表現すると以下のようになります。
$$
y = \begin{cases}
0 & (b + w_1 \cdot x_1 + w_2 \cdot x_2 \leq 0) \\
1 & (b + w_1 \cdot x_1 + w_2 \cdot x_2 > 0)
\end{cases}
$$
ここで、$w_1$ と $w_2$ は重み、$b$ はバイアス、$x_1$ と $x_2$ は入力値です。

## 第3章 ニューラルネットワークの基礎

### シグモイド関数
シグモイド関数は、入力を0から1の範囲に圧縮する活性化関数です。
$$
h(x) = \frac{1}{1 + \exp(-x)}
$$
この関数は、ニューラルネットワークの出力を確率として解釈する際に使用されます。

### ReLU関数
ReLU (Rectified Linear Unit) 関数は、入力が0以下のときに0を出力し、入力が0以上のときにそのままの値を出力します。
$$
h(x) = \max(0, x)
$$
この関数は、ニューラルネットワークの学習を高速化するために広く使用されています。

## 第4章 ニューラルネットワークの学習

### 二乗和誤差
二乗和誤差（Mean Squared Error, MSE）は、予測値と実際の値の差の二乗和を平均したものです。
$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$
ここで、$y_k$ は予測値、$t_k$ は実際の値です。

### 交差エントロピー誤差
交差エントロピー誤差（Cross Entropy Error）は、分類問題で使用される損失関数です。
$$
E = - \sum_{k} t_k \log y_k
$$
ここで、$y_k$ は予測確率、$t_k$ は実際のクラスラベルです。

## 第5章 誤差逆伝播法

### 誤差逆伝播法の基本式
誤差逆伝播法は、出力層から入力層に向かって誤差を伝播させ、各層の重みを更新する手法です。
$$
\frac{\partial E}{\partial w_{ij}^{(l)}} = \delta_j^{(l)} h_i^{(l-1)}
$$
ここで、$E$ は損失関数、$w_{ij}^{(l)}$ は第$l$層の重み、$\delta_j^{(l)}$ は第$l$層の$j$番目のノードの誤差、$h_i^{(l-1)}$ は第$(l-1)$層の$i$番目のノードの出力です。

## 第6章 学習に関するテクニック

### 勾配降下法
勾配降下法は、損失関数の勾配を用いてパラメータを更新する手法です。
$$
w = w - \eta \frac{\partial E}{\partial w}
$$
ここで、$w$ はパラメータ、$\eta$ は学習率、$E$ は損失関数です。

## 第7章 畳み込みニューラルネットワーク

### 畳み込み演算
畳み込み演算は、フィルタを用いて入力データ（例：画像）から特徴を抽出する手法です。
$$
z_{ij} = (x * w)_{ij} = \sum_{m} \sum_{n} x_{i+m, j+n} w_{mn}
$$
ここで、$x$ は入力データ、$w$ はフィルタ、$z_{ij}$ は出力データの$(i, j)$成分です。

## 第8章 再帰型ニューラルネットワーク

### RNNの順伝播
再帰型ニューラルネットワーク（RNN）は、時系列データに適したモデルです。RNNの順伝播は以下の式で表されます。
$$
h_t = \tanh(W_h h_{t-1} + W_x x_t + b)
$$
ここで、$h_t$ は時刻$t$の隠れ状態、$W_h$ と $W_x$ は重み、$x_t$ は時刻$t$の入力データ、$b$ はバイアスです。

## まとめ
このドキュメントでは、「ゼロから作るディープラーニング」に登場する主要な数式について解説しました。各数式の意味とその役割を理解することで、ディープラーニングの理論と実装をより深く理解することができます。
